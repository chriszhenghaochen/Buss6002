{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>BUSS6002 - Data Science in Business</h1></center>\n",
    "\n",
    "#### Pre-Tutorial Checklist\n",
    "\n",
    "1. Complete all tasks from week 9\n",
    "2. Read up to exercise 1\n",
    "3. Install mockr library\n",
    "\n",
    "\n",
    "# Tutorial 10 - MapReduce\n",
    "\n",
    "\n",
    "## What is MapReduce? \n",
    "\n",
    "MapReduce is a programming model that allows us to perform calculations in parallel rather than in series, so that instead of requiring a supercomputer to iterate through all calculations extremely fast, we can instead have lots of regular processors working on parts of a problem at the same time. MapReduce is composed of two steps. The Mapping phase breaks the problem into smaller independent calculations that can be solved individually and maps them to queues. The Reducing phase combines the outcomes of these smaller computations to arrive at a solution for the original task. MapReduce is most often used for processing data, however it is flexible enough that it can be used for more complex tasks such as linear regression, which we will demonstrate.\n",
    "\n",
    "A MapReduce program consists of two primary steps:\n",
    "\n",
    "**Map**, performs filtering and sorting into queues\n",
    "\n",
    "**Reduce**, performs a summary operation on a queue\n",
    "\n",
    "<img src=\"img/mapreduce.gif\">\n",
    "\n",
    "*Image sourced from https://www.ibm.com/developerworks/cloud/library/cl-openstack-deployhadoop/*\n",
    "\n",
    "## Why Do We Need MapReduce?\n",
    "\n",
    "As the volume of data acquired by businesses and individuals continues to grow it becomes problematic to store and process this data rapidly enough for useful insights. Most naive algorithms cannot deal with such volumes of data, either due to requiring too much computation time or memory.\n",
    "\n",
    "The \"divide and conquer\" style of MapReduce has the ability to resolve this issue. By carefully decomposing the problem we can distribute the processing load over many machines. This is known as horizontal scaling. This distrubuted computing style is more robust as we are no longer reliant on one large computer for our processing. MapReduce implementations can be designed in a fault tolerant way so that machines can be disabled or disconnected without affecting results. Additionally the hardware requirements of each machine is reduced.\n",
    "\n",
    "## MapReduce in Python\n",
    "\n",
    "There are many MapReduce libraries in Python. We will use \"mockr\", which is a very simple library.\n",
    "\n",
    "mockr allows us to write our map and reduce functions in normal Python. You can install it via:\n",
    "\n",
    "    pip install mockr\n",
    "    \n",
    "<div style=\"margin-bottom: 0px;\"><img width=20 style=\"display: block; float: left;  margin-right: 20px;\" src=\"img/docs.png\"> <h3 style=\"padding-top: 0px;\">Documentation - mockr</h3></div>\n",
    "https://pypi.org/project/mockr/\n",
    "    \n",
    "\n",
    "## Example: Counting Words\n",
    "\n",
    "Lets start by using the canonical MapReduce example of counting words in a corpus.\n",
    "\n",
    "First we need to define the map and reduce functions. Then we can run the mapreduce algorithm to count the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "This is a sample string.\n",
      "It is very simple.\n",
      "Goodbye!\n",
      "#-----------------------#\n",
      "result\n",
      "[('hello', 1), ('this', 1), ('is', 2), ('a', 1), ('sample', 1), ('string', 1), ('it', 1), ('very', 1), ('simple', 1), ('goodbye', 1)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from mockr import run_stream_job\n",
    "\n",
    "# This regular expression matches words\n",
    "# Test it at https://www.regexpal.com\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # that means all the words that characters and numbers\n",
    "\n",
    "# think: if you have n words with k unique words as input\n",
    "# map function\n",
    "def map_fn(chunk):\n",
    "    # Use the regex to find all words in each chunk\n",
    "    # The chunk is a line of text because we are\n",
    "    # using run_stream_job\n",
    "    \n",
    "    # find all substring that divided by regex (e.g. ' ', '#!@#!@', '$%@#$!!')\n",
    "    for word in WORD_RE.findall(chunk):\n",
    "        # Emit a result using the word as the key and\n",
    "        # the number of times it occured. We emit once\n",
    "        # for each word so this value is 1.\n",
    "        yield (word.lower(), 1)\n",
    "        \n",
    "# reduce function\n",
    "def reduce_fn(key, values):\n",
    "    # Recieves all the values for each key (unique word)\n",
    "    # then sums them together for the total count\n",
    "    yield (key, sum(values))\n",
    "\n",
    "# \"\\n\" is the newline character which seperates the lines of text\n",
    "input_str = \"Hello!\\nThis is a sample string.\\nIt is very simple.\\nGoodbye!\"\n",
    "print(input_str)\n",
    "\n",
    "# run_stream_job expects a newline delimited string, map function and reduce function\n",
    "# and returns a list of results\n",
    "# https://mockr.readthedocs.io/en/latest/api.html#mockrmockmr.run_stream_job\n",
    "\n",
    "# this run_stream_job function is to warp the map and reduce function\n",
    "results = run_stream_job(input_str, map_fn, reduce_fn)\n",
    "print('#-----------------------#')\n",
    "print(\"result\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the 1 times loop for yield generator\n",
      "0\n",
      "1\n",
      "4\n",
      "this is the 2 times loop for yield generator\n",
      "this is the 3 times loop for yield generator\n",
      "this is the 4 times loop for yield generator\n",
      "this is the 5 times loop for yield generator\n",
      "this is the 6 times loop for yield generator\n",
      "this is the 7 times loop for yield generator\n",
      "this is the 8 times loop for yield generator\n",
      "this is the 9 times loop for yield generator\n",
      "this is the 10 times loop for yield generator\n",
      "\n",
      "####################################\n",
      "\n",
      "this is the 1times loop for list\n",
      "0\n",
      "1\n",
      "2\n",
      "this is the 2times loop for list\n",
      "0\n",
      "1\n",
      "2\n",
      "this is the 3times loop for list\n",
      "0\n",
      "1\n",
      "2\n",
      "this is the 4times loop for list\n",
      "0\n",
      "1\n",
      "2\n",
      "this is the 5times loop for list\n",
      "0\n",
      "1\n",
      "2\n",
      "this is the 6times loop for list\n",
      "0\n",
      "1\n",
      "2\n",
      "this is the 7times loop for list\n",
      "0\n",
      "1\n",
      "2\n",
      "this is the 8times loop for list\n",
      "0\n",
      "1\n",
      "2\n",
      "this is the 9times loop for list\n",
      "0\n",
      "1\n",
      "2\n",
      "this is the 10times loop for list\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# yield is the thing to create the 1-time generator for a loop\n",
    "# list is also a iterative function, but you can use it infinitely\n",
    "\n",
    "# generator\n",
    "def generator():\n",
    "    data = range(3) # create a list [0,1,2]\n",
    "    for i in data:\n",
    "        yield i*i # store i*i into my generator using yeild\n",
    "        \n",
    "g = generator()\n",
    "li = [0,1,2]\n",
    "\n",
    "for j in range(10):\n",
    "    print('this is the ' + str(j+1) + ' times loop for yield generator')\n",
    "    for i in g:\n",
    "        print(i)\n",
    "\n",
    "print()\n",
    "print('####################################')\n",
    "print()  \n",
    "\n",
    "# that shows that your list can loop as many as you want\n",
    "for j in range(10):\n",
    "    print('this is the ' + str(j+1) + 'times loop for list')\n",
    "    for i in li:\n",
    "        print(i)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "I\n",
      "am\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "for word in WORD_RE.findall('hello $I @#! am !@##$ here'):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yield\n",
    "\n",
    "The yield keyword returns an ephemeral object. Once it is read by any other code it gets deleted immediately. We use yield so that the data is guaranteed to only be read by one of the reducers.\n",
    "\n",
    "<div style=\"margin-bottom: 30px;\"><img width=48 style=\"display: block; float: left;  margin-right: 20px;\" src=\"img/question-mark-button.png\"> <h3 style=\"padding-top: 15px;\">Exercise 1 - Wordcount MapReduce</h3></div>\n",
    "\n",
    "Given a line of text that's n words long with k unique words as input:\n",
    "- How many keys are returned by the `map_fn` function? \n",
    "\n",
    "answer: n\n",
    "\n",
    "- How many keys are returned after the output from `map_fn` has been passed through `reduce_fn`?\n",
    "\n",
    "answer: k\n",
    "\n",
    "## Explanation\n",
    "\n",
    "1. Each line of text is sent to a mapper\n",
    "\n",
    "\n",
    "2. Each mapper:\n",
    "    - Splits the line into words\n",
    "    - For each word: yields a key/value pair\n",
    "    - Key: word\n",
    "    - Value: number of times it occurs (since we iterate over all words we set this to 1)\n",
    "\n",
    "\n",
    "3. Mapped key/value pairs are sent to shuffler\n",
    "\n",
    "\n",
    "4. Key/value pairs are shuffled\n",
    "    - For each key collect the corresponding list of values\n",
    "\n",
    "\n",
    "5. Key/values are routed to respective reducers\n",
    "    - Each reducer only works on a single key\n",
    "\n",
    "\n",
    "6. Reduce\n",
    "    - Sum the counts for a key\n",
    "\n",
    "## Simulation of MapReduce Stages\n",
    "\n",
    "Below is a simulation of the output at each stage.\n",
    "\n",
    "### Text is broken into lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!', 'This is a sample string.', 'It is very simple.', 'Goodbye!']\n"
     ]
    }
   ],
   "source": [
    "# 1st step: split whole into subdata before input them into the map\n",
    "input_str = \"Hello!\\nThis is a sample string.\\nIt is very simple.\\nGoodbye!\"\n",
    "\n",
    "# print(input_str)\n",
    "\n",
    "# Split the string into lines and store in a list\n",
    "lines_of_text = input_str.split(\"\\n\")\n",
    "\n",
    "# #e.g.\n",
    "# yolo = 'hello ella how are you'.split('e')\n",
    "# print(yolo)\n",
    "\n",
    "print(lines_of_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping\n",
    "\n",
    "We need to apply map_fn to each line of text and collect the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 1)]\n",
      "[('this', 1), ('is', 1), ('a', 1), ('sample', 1), ('string', 1)]\n",
      "[('it', 1), ('is', 1), ('very', 1), ('simple', 1)]\n",
      "[('goodbye', 1)]\n"
     ]
    }
   ],
   "source": [
    "# We will store the output of map_fn in here\n",
    "word_count_lists = []\n",
    "\n",
    "# For every line of text\n",
    "for line in lines_of_text:\n",
    "    # Apply the map function (split and count words)\n",
    "    # Save the result as a list in our list\n",
    "    word_count_lists.append(list(map_fn(line)))\n",
    "\n",
    "# # still remember what the map_func does?\n",
    "# def map_fn(chunk):\n",
    "#     # Use the regex to find all words in each chunk\n",
    "#     # The chunk is a line of text because we are\n",
    "#     # using run_stream_job\n",
    "    \n",
    "#     # find all substring that divided by regex (e.g. ' ', '#!@#!@', '$%@#$!!')\n",
    "#     for word in WORD_RE.findall(chunk):\n",
    "#         # Emit a result using the word as the key and\n",
    "#         # the number of times it occured. We emit once\n",
    "#         # for each word so this value is 1.\n",
    "#         yield (word.lower(), 1) # this is nothing different from append, except it can only be used once\n",
    "    \n",
    "# Show the result of mapping\n",
    "for words in word_count_lists:\n",
    "    print(words)\n",
    "# print(word_count_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nested list of lists, which we can flatten into a normal list using itertools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 1), ('this', 1), ('is', 1), ('a', 1), ('sample', 1), ('string', 1), ('it', 1), ('is', 1), ('very', 1), ('simple', 1), ('goodbye', 1)]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# word_count_lists is a list of lists\n",
    "# Flatten the list of words to make it simpler by chaining lists together\n",
    "word_count_list_flat = list(itertools.chain.from_iterable(word_count_lists))\n",
    "\n",
    "print(word_count_list_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle and Sort\n",
    "\n",
    "This stage is taken care of by mockr but it is important to understand!\n",
    "\n",
    "We need to group all the counts for each word together. We will store this in a dictionary (key/value data type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello: [('hello', 1)]\n",
      "this: [('this', 1)]\n",
      "is: [('is', 1), ('is', 1)]\n",
      "a: [('a', 1)]\n",
      "sample: [('sample', 1)]\n",
      "string: [('string', 1)]\n",
      "it: [('it', 1)]\n",
      "very: [('very', 1)]\n",
      "simple: [('simple', 1)]\n",
      "goodbye: [('goodbye', 1)]\n"
     ]
    }
   ],
   "source": [
    "# SHUFFLE/SORT STAGE\n",
    "from collections import defaultdict\n",
    "\n",
    "# # Create a dictionary where the default value is a list\n",
    "word_tuple_dict = defaultdict(list)\n",
    "# # print(word_tuple_dict)\n",
    "\n",
    "for kv_pair in word_count_list_flat:\n",
    "#     print(kv_pair)\n",
    "    # For each unique key append the (word, count) tuple to that keys list\n",
    "    word_tuple_dict[kv_pair[0]].append(kv_pair)\n",
    "\n",
    "# Print it in a nice format:\n",
    "for k, v in word_tuple_dict.items():\n",
    "    print(str(k) +\": \" + str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "\n",
    "Now we need to apply the reduce_fn to our sorted data. Collect all counts for each word into a list and send this to the reduce_function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('hello', 1)], [('this', 1)], [('is', 2)], [('a', 1)], [('sample', 1)], [('string', 1)], [('it', 1)], [('very', 1)], [('simple', 1)], [('goodbye', 1)]]\n"
     ]
    }
   ],
   "source": [
    "# REDUCE STAGE\n",
    "results = []\n",
    "\n",
    "for k, v in word_tuple_dict.items():\n",
    "    # Get the counts from the list of k/v pairs\n",
    "    vals_list = [t[1] for t in v]\n",
    "    \n",
    "    # Apply the reduce_fn to the word and counts pair\n",
    "    # reduce_fn will yield a (key, value) tuple\n",
    "    # inside a generator object which we convert to a list\n",
    "    results.append(list(reduce_fn(k, vals_list)))\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we flatten the list using itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 1), ('this', 1), ('is', 2), ('a', 1), ('sample', 1), ('string', 1), ('it', 1), ('very', 1), ('simple', 1), ('goodbye', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Flatten the results to make them more readable\n",
    "results_flat = list(itertools.chain.from_iterable(results))\n",
    "\n",
    "print(results_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data Set\n",
    "\n",
    "This was a small example. Let's look at a bigger example: counting all the words in Moby Dick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mockr import run_stream_job\n",
    "\n",
    "# Open the MobyDick text file\n",
    "input_file = open(\"MobyDick.txt\", 'r')\n",
    "\n",
    "# Read the file as a string\n",
    "input_str = input_file.read()\n",
    "\n",
    "# print(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 14697), ('project', 91), ('gutenberg', 92), ('ebook', 10), ('of', 6742), ('moby', 89), ('dick', 88), ('or', 797), ('whale', 1101), ('by', 1222), ('herman', 4), ('melville', 4), ('this', 1438), ('is', 1746), ('for', 1643), ('use', 49), ('anyone', 6), ('anywhere', 16), ('at', 1335), ('no', 591), ('cost', 4), ('and', 6515), ('with', 1769), ('almost', 197), ('restrictions', 2), ('whatsoever', 7), ('you', 938), ('may', 254), ('copy', 19), ('it', 2431), ('give', 90), ('away', 186), ('re', 7), ('under', 126), ('terms', 33), ('license', 18), ('included', 14), ('online', 4), ('www', 6), ('org', 13), ('title', 8), ('author', 9), ('release', 1), ('date', 4), ('december', 4), ('25', 4), ('2008', 1), ('2701', 4), ('last', 278), ('updated', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Run map reduce using the same map_fn and reduce_fn that we defined before\n",
    "results = run_stream_job(input_str, map_fn, reduce_fn)\n",
    "\n",
    "# Show the first 20 results\n",
    "print(results[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling \n",
    "\n",
    "Imagine we want to perform this same process on all the tweets ever generated. Or on all the text on the web. Can we do this with your personal computer in a reasonable time? No! We would need to use a cluster of computers (workers) and map-reduce.\n",
    "\n",
    "With the previous word count example we could send any line of text to any map worker in the cluster. Then we can have a set of reduce workers dedicated to one or many keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Linear Regression\n",
    "\n",
    "Many machine learning problems can be decomposed into sub problems that can be solved independently. This means we can use map-reduce to solve them.\n",
    "\n",
    "Recall from week 6 that we can calculate the optimal coefficients for linear regression explicitly using linear algebra. For feature matrix X:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}1&x_{11}&x_{12}&...&x_{1p}\\\\1&x_{21}&x_{22}&...&x_{2p}\\\\ & &...\\\\1&x_{n1}&x_{n2}&...&x_{np}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And label vector y:\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}y_1\\\\y_2\\\\...\\\\y_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can use the formula\n",
    "\n",
    "$$ \\beta = \\left( X^T X\\right)^{-1} X^T \\mathbf y$$\n",
    "\n",
    "The trick to using MapReduce for this problem is noticing that $X^TX$ and $X^Ty$ for the whole dataset can be written as the sum of $X_i^TX_i$ and $X_i^Ty_i$ respectively, where the subscript i refers to individual observations, or subsets of observations.\n",
    "\n",
    "We can decompose Linear Regression by dividing the data into subsets and computing the major matrix multiplication on each of the set. These are later recombined and inverted. See the slides for more details.\n",
    "\n",
    "To pass data in mapreduce it must be in plaintext. So we have to convert our usual DataFrame to a text format such as JSON between each step.\n",
    "\n",
    "The following code outputs the estimated parameters of a Linear Regression on the BatonRouge dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2,3],[1,2,3]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# (Xi^T Xi, Xi^TYi)\n",
    "def map_linear_fn(chunk):\n",
    "    # Get the dependant variable y\n",
    "    # label y\n",
    "    y = chunk['Price'].values\n",
    "\n",
    "    # Get the independant/feature variables\n",
    "    # which is everything except the price column\n",
    "    X_vals = chunk[chunk.columns.difference(['Price', 'Style'])].values\n",
    "\n",
    "    # Get the number of data points\n",
    "    m = chunk.shape[0]\n",
    "\n",
    "    # Insert a column of \"1\"s for the intercept term \n",
    "    # X = [2,2,3] => X = [1,2,2,3]\n",
    "    X = np.column_stack((np.ones(m), X_vals))\n",
    "\n",
    "    # Convert to matrix to make multiplication easier\n",
    "    X = np.asmatrix(X)\n",
    "\n",
    "    # Calculate required multiplications\n",
    "    XtX = X.T*X\n",
    "    Xty = X.T * y.reshape(m,1)\n",
    "\n",
    "    # Yield the result\n",
    "    yield(\"result\", [XtX, Xty])\n",
    "\n",
    "# supposely, values should compose with (XtX, Xty)\n",
    "def reduce_linear_fn(key, values):\n",
    "\n",
    "    # Create lists to accumulate the matrices/vectors in\n",
    "    XtX_list = []\n",
    "    Xty_list = []\n",
    "\n",
    "    for result_list in values:\n",
    "        XtX_list.append(result_list[0])\n",
    "        Xty_list.append(result_list[1])\n",
    "\n",
    "    # Sum up all the XtX matrices\n",
    "    XtX = np.asmatrix(sum(XtX_list))\n",
    "\n",
    "    # Sum up all the Xty vectors\n",
    "    Xty = sum(Xty_list)\n",
    "\n",
    "    # Solve the linear regression objective inv is inverse i.e. (^-1)\n",
    "    betas = np.linalg.inv(XtX) * Xty\n",
    "\n",
    "    yield (key, betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mockr import run_pandas_job\n",
    "\n",
    "df = pd.read_excel(\"BatonRouge.xls\")\n",
    "# print(df)\n",
    "\n",
    "# df = df.drop(columns=['Style']).astype('float32')\n",
    "\n",
    "results = run_pandas_job(df, map_linear_fn, reduce_linear_fn, n_chunks = 4)\n",
    "# each single element is the P-i for P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('result', matrix([[-4.93453976e+04],\n",
      "        [-4.30636024e+02],\n",
      "        [ 4.01560091e+04],\n",
      "        [-2.69227757e+04],\n",
      "        [-2.25516565e+01],\n",
      "        [-2.59406734e+03],\n",
      "        [ 6.92537050e+03],\n",
      "        [-2.50175666e+03],\n",
      "        [ 8.54116998e+01],\n",
      "        [ 1.63298319e+01],\n",
      "        [ 5.57871062e+04]]))]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with sklearn parameters\n",
    "\n",
    "Note that even though we seperated the data into multiple chunks the final result is exactly the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.93453976e+04 -4.30636024e+02  4.01560091e+04 -2.69227757e+04\n",
      " -2.25516565e+01 -2.59406734e+03  6.92537050e+03 -2.50175666e+03\n",
      "  8.54116998e+01  1.63298319e+01  5.57871062e+04]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_obj = LinearRegression()\n",
    "\n",
    "features = df[df.columns.difference(['Price', 'Style'])]\n",
    "target = df['Price']\n",
    "\n",
    "# training function\n",
    "lr_obj.fit(features, target)\n",
    "\n",
    "params_sk = np.append(np.array(lr_obj.intercept_), lr_obj.coef_)\n",
    "\n",
    "print(params_sk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.93453976e+04 -4.30636024e+02  4.01560091e+04 -2.69227757e+04\n",
      " -2.25516565e+01 -2.59406734e+03  6.92537050e+03 -2.50175666e+03\n",
      "  8.54116998e+01  1.63298319e+01  5.57871062e+04]\n"
     ]
    }
   ],
   "source": [
    "# Simplify the result to make it more readable\n",
    "params_mr = np.array(results[0][1]).ravel() # flat the array\n",
    "\n",
    "print(params_mr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-bottom: 30px;\"><img width=48 style=\"display: block; float: left;  margin-right: 20px;\" src=\"img/question-mark-button.png\"> <h3 style=\"padding-top: 15px;\">Exercise 2 - Linear Regression and MapReduce</h3></div>\n",
    "\n",
    "In the equation $ \\beta = \\left( X^T X\\right)^{-1} X^T \\mathbf y$:\n",
    "\n",
    "- What are the dimensions of $X^TX$?\n",
    "- What about $X^Ty$?\n",
    "\n",
    "2. Check using Numpy that the product $$X^TX = \\begin{bmatrix}1&2&3\\\\ 4&5&6\\end{bmatrix}\\begin{bmatrix}1&4\\\\ 2&5\\\\ 3&6\\end{bmatrix}$$ can be written as the sum \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}3\\\\ 6\\end{bmatrix} \\begin{bmatrix}3& 6\\end{bmatrix}\n",
    "+ \\begin{bmatrix}2\\\\ 5\\end{bmatrix} \\begin{bmatrix}2& 5\\end{bmatrix}\n",
    "+ \\begin{bmatrix}1\\\\ 4\\end{bmatrix} \\begin{bmatrix}1& 4\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3-5.0.1x64\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (10,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance</th>\n",
       "      <th>Delayed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>308.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>296.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>480.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>296.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>373.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Distance  Delayed\n",
       "0     308.0        0\n",
       "1     296.0        0\n",
       "2     480.0        1\n",
       "3     296.0        0\n",
       "4     373.0        0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Import airline dataset\n",
    "airline = pd.read_csv('airline_small.csv', encoding='ISO-8859-1')\n",
    "\n",
    "#Create delayed column (1 if ArrDelay > 20, 0 otherwise)\n",
    "airline['Delayed'] = (airline['ArrDelay'] > 20).astype(int)\n",
    "\n",
    "#Subset to two columns and drop null values\n",
    "sub_airline = airline[['Distance', 'Delayed']]\n",
    "sub_airline = sub_airline.dropna()\n",
    "\n",
    "sub_airline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def map_linear_fn(chunk):\n",
    "    # Get the dependant variable y\n",
    "    # label, \n",
    "    y = chunk['Delayed'].values\n",
    "\n",
    "    # Get the independant/feature variables\n",
    "    # which is everything except the price column\n",
    "    x = chunk['Distance'].values.reshape(-1, 1)\n",
    "\n",
    "    # Initialize our logisic regression model and then fit it\n",
    "    log_reg = LogisticRegression(solver = 'lbfgs')\n",
    "    \n",
    "    # train sub-set of data\n",
    "    log_reg.fit(x, y)\n",
    "\n",
    "    # Combine the intercept and the coefficient together, because together they are the parameters of our model\n",
    "    # Those are all the parameters of our Logistic Regression Model\n",
    "    parameters = np.append(np.array(log_reg.intercept_), log_reg.coef_)\n",
    "\n",
    "    # Return the actual parameter\n",
    "    # Yield is the same as return, but the returned result can ONLY BE USED ONCE\n",
    "    #print('beta:')\n",
    "    #print(parameters)\n",
    "    yield ('result', parameters) # pi for subset data i\n",
    "\n",
    "def reduce_linear_fn(key, values):\n",
    "\n",
    "    # The reducer's job is just to average all the results from the independent jobs together\n",
    "    # A generator just means the same thing as a list BUT it can only be accessed once\n",
    "\n",
    "    # Store all the results into a list, and then we will average all of them together\n",
    "    param_list = []\n",
    "    for param in values:\n",
    "        param_list.append(np.array(param))\n",
    "\n",
    "    # Average all the parameters together\n",
    "    #mean (p0, p1, p2, p3) = > p\n",
    "    avg_params = np.mean(param_list, axis=0)\n",
    "\n",
    "    # Return the final result\n",
    "    yield key, avg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('result', array([ -1.88075578e+00,   1.76807716e-04]))]\n"
     ]
    }
   ],
   "source": [
    "from mockr import run_pandas_job\n",
    "results = run_pandas_job(sub_airline, map_linear_fn, reduce_linear_fn, n_chunks =100)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.80835051e+00   1.39805132e-04]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y = sub_airline['Delayed']\n",
    "\n",
    "x = sub_airline['Distance'].values.reshape(-1,1)\n",
    "\n",
    "log_res = LogisticRegression()\n",
    "\n",
    "log_res.fit(x, y)\n",
    "\n",
    "params = np.append(np.array(log_res.intercept_), log_res.coef_)\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
